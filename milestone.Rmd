---
title: "Capstone Project - Milestone Report"
author: "Priscilla Kurtz"
date: "05/30/2020"
output: html_document
---

# Synopsis

The Capstone Project objective is to build a predictive text model from [HC Corpora](http://www.corpora.heliohost.org) database. The model should suggest the next word based on the previous words used. The text documents are provided from different three web sources: blogs, twitter and news articles. This report shows the first exploration on the data.

Libraries used for this exploration:

```{r message=FALSE, warning=FALSE, include=FALSE}
library(RCurl)
library(jsonlite)
library(textcat)
library(parallel)
library(tm)
library(RColorBrewer)
library(wordcloud)
library(stringi)
library(ggplot2)
library(openNLP)
library(textmineR)
library(RWeka)
library(dplyr)
```

# Download data and load it in

I've downloaded the Capstone Dataset from the URL given at [Task 0](https://class.coursera.org/dsscapstone-003/wiki/Task_0) page. It was compressed, I've decompressed it manually and put it inside a directory. 

There are four languages for each sample files - German, English, Finnish and Russian. Each sample is composed of three files: one from blogs, another from twitter and the last one with data attached to news. I'll focus only on the English files.

```{r, cache=TRUE, warning=FALSE}
directory <- "C:\\Users\\Home\\Documents\\capstone\\dataset\\en_US\\"

blog.file <- paste(directory,"en_US.blogs.txt", sep = "")
twitter.file <- paste(directory,"en_US.twitter.txt", sep = "")
news.file <- paste(directory,"en_US.news.txt", sep = "")

blog <- readLines(blog.file, encoding="UTF-8")
twitter <- readLines(twitter.file, encoding="UTF-8")
news <- readLines(news.file, encoding="UTF-8")
```

# Basic report of summary statistics about the data sets

```{r, cache=TRUE, warning=FALSE}
blog.stats <- stri_stats_general(blog)
twitter.stats <- stri_stats_general(twitter)
news.stats <- stri_stats_general(news)

blog.summary <- summary(sapply(blog,FUN=nchar))
twitter.summary <- summary(sapply(twitter,FUN=nchar))
news.summary <- summary(sapply(news,FUN=nchar))

blog.wordCount <- summary(stri_count_words(blog))
twitter.wordCount <- summary(stri_count_words(twitter))
news.wordCount <- summary(stri_count_words(news))
```

Here are some stats of our data:

File | blogs | twitter | news
-----|-------|---------|-----
Size| `r round(file.info(blog.file)$size / 1024 / 1024,2)` MB | `r round (file.info(twitter.file)$size / 1024 / 1024,2)` MB| `r round (file.info(news.file)$size / 1024 / 1024,2)` MB
Lines|`r blog.stats[["Lines"]]`|`r twitter.stats[["Lines"]]`|`r news.stats[["Lines"]]`
Mean Character Per Line| `r round(blog.summary[["Mean"]],2)` | `r round(twitter.summary[["Mean"]],2)` | `r round(news.summary[["Mean"]],2)`
Mean Word Count| `r round(blog.wordCount[["Mean"]],2)` | `r round(twitter.wordCount[["Mean"]],2)` | `r round(news.wordCount[["Mean"]],2)`

# Data Cleanning and Preprocessing

Since files are too big and to build the model we don't need to work with all of it, I've created one sample object with three samples from our data to build our corpora: 

```{r , cache=TRUE, warning=FALSE}
blog.sample <- head(blog, 3000)
twitter.sample <- head(twitter, n=3000)
news.sample <- head(news, 3000)
sample <- iconv(as.String(paste(twitter.sample, news.sample, blog.sample, collapse = " ")))
```

We still need to remove punctuation, numbers, stop words and strip any whitespace. As a normalization step, I've lowered cases for all words. To build our corpora we use _tm_ package.

```{r, cache=TRUE, warning=FALSE}
corpus <- VCorpus(VectorSource(sample))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removePunctuation))
corpus <- tm_map(corpus, content_transformer(removeNumbers))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, function(x) removeWords(x, stopwords("english")))
```

Let's study the frequencies of words in our corpora using 1-gram, 2-gram and 3-gram.

```{r}
ngram <- function(x,n) NGramTokenizer(x, Weka_control(min = n, max = n))
n1gram <- function(x) ngram(x,1)
n2gram <- function(x) ngram(x,2)
n3gram <- function(x) ngram(x,3)
corpus.tdm <- TermDocumentMatrix(corpus)
corpus.tdm.n1 <- TermDocumentMatrix(corpus, control = list(tokenize = n1gram))
corpus.tdm.n2 <- TermDocumentMatrix(corpus, control = list(tokenize = n2gram))
corpus.tdm.n3 <- TermDocumentMatrix(corpus, control = list(tokenize = n3gram))
corpus.tdm.m <- as.matrix(corpus.tdm)
corpus.tdm.v <- sort(rowSums(corpus.tdm.m),decreasing=TRUE)
```

## 1-gram

```{r}
unigram <- data.frame(Term = corpus.tdm.n1$dimnames$Terms, 
                      Freq = corpus.tdm.n1$v) 
unigram <- unigram[order(unigram$Freq,decreasing = T),] 

unigram.cumsum <- cumsum(unigram$Freq)
limit50 <- sum(unigram$Freq)*0.5
limit90 <- sum(unigram$Freq) * 0.9

ggplot(head(unigram,20), aes(x=reorder(Term,-Freq), y=Freq)) +
  geom_bar(stat="Identity", fill="green") +
  geom_text(aes(label=Freq), vjust = -0.5) +
  ggtitle("1-gram frequency plot") +
  ylab("Frequency") +
  xlab("Term")+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

It's easy to see through the frequency plot that frequency tends to decrease and to stabilize very fast. It makes sense because the frequency you use one word is largely bigger than two words together, and so on.

We have `r nrow(unigram)` unique words on this sample, and `r nrow(unigram %>% filter(Freq == 1))` words occurs only one time.

We need `r length((unigram.cumsum[unigram.cumsum <=  limit50]))` to cover 50% of the instances and `r length((unigram.cumsum[unigram.cumsum <=  limit90]))` words to cover 90% of the instances.

## 2-gram

```{r}
bigram <- data.frame(Term = corpus.tdm.n2$dimnames$Terms, 
                     Freq = corpus.tdm.n2$v) 
bigram <- bigram[order(bigram$Freq,decreasing = T),] 

bigram.cumsum <- cumsum(bigram$Freq)
bigram.limit50 <- sum(bigram$Freq)*0.5
bigram.limit90 <- sum(bigram$Freq) * 0.9

ggplot(head(bigram,20), aes(x=reorder(Term,-Freq), y=Freq)) +
  geom_bar(stat="Identity", fill="yellow") +
  geom_text(aes(label=Freq), vjust = -0.5) +
  ggtitle("2-gram frequency plot") +
  ylab("Frequency") +
  xlab("Term") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

We have `r nrow(bigram)` unique pair of words on this sample but `r nrow(bigram %>% filter(Freq == 1))` pair of words occurs only one time.

We need `r length((bigram.cumsum[bigram.cumsum <=  limit50]))` to cover 50% of the instances and `r length((bigram.cumsum[bigram.cumsum <= limit90]))` words to cover 90% of the instances.

## 3-gram

```{r}
threegram <- data.frame(Term = corpus.tdm.n3$dimnames$Terms, Freq = corpus.tdm.n3$v) 
threegram <- threegram[order(threegram$Freq,decreasing = T),] 

threegram.cumsum <- cumsum(threegram$Freq)
threegram.limit50 <- sum(threegram$Freq)*0.5
threegram.limit90 <- sum(threegram$Freq) * 0.9

ggplot(head(threegram,15), aes(x=reorder(Term,-Freq), y=Freq)) +
  geom_bar(stat="Identity",fill="blue") +
  geom_text(aes(label=Freq), vjust = -0.5) +
  ggtitle("3-gram frequency plot") +
  ylab("Frequency") +
  xlab("Term") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

We have `r nrow(threegram)` unique set of three words on this sample but `r nrow(threegram %>% filter(Freq == 1))` set of three words occurs only one time.

We need `r length((threegram.cumsum[threegram.cumsum <=  limit50]))` to cover 50% of the instances and `r length((threegram.cumsum[threegram.cumsum <= limit90]))` words to cover 90% of the instances.

# Next Steps

* Build prediction model
    + Remove profanity
    + Research how to evaluate the model.
    + Test algorithms that can predict probability for more than one response, so we can offer alternatives to the user.
    + Use 2-gram, 3-gram, 4-gram and 5-gram to predict next word.
    + Use [Katz's back-off model](http://en.wikipedia.org/wiki/Katz's_back-off_model).
* Create shinny app
    + Consider the size in memory and performance (runtime) of the model.
    + Develop and publish the app in shinyapps.io and check the resources consuption.
    + Let some friend test the app and give some feedback.
* Create a cool presentation